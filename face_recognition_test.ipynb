{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f561e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    RUNNING_IN_COLAB = True\n",
    "    \n",
    "except ImportError:\n",
    "    drive = None\n",
    "    RUNNING_IN_COLAB = False\n",
    "\n",
    "if RUNNING_IN_COLAB:\n",
    "    os.system('git clone https://github.com/vekshinnikita/face_recognition.git /content/face_recognition')\n",
    "    os.chdir('/content/face_recognition') \n",
    "    \n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    zip_dataset_path = '/content/drive/MyDrive/recognition_dataset.zip'\n",
    "    destination_dataset_path = '/content/face_recognition/'\n",
    "    with zipfile.ZipFile(zip_dataset_path, 'r') as zip_ref:\n",
    "      zip_ref.extractall(destination_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d296507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Documents/university/Дипломы/Никита/project/venv_yolo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, using CPU instead\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from utils.system import get_available_device\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "\n",
    "device = get_available_device()\n",
    "\n",
    "KNOWN_FACES_PATH='./known_faces'\n",
    "\n",
    "# Model\n",
    "detector_model = YOLO(\"./best_models/yolo_decoder_best.pt\", verbose=False).to(device)\n",
    "classifier_model = InceptionResnetV1(pretrained='vggface2', num_classes=2, device=device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37abeffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_embeddings = list()\n",
    "class_embedding_names = list()\n",
    "\n",
    "index = faiss.IndexFlatL2(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb88a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.uint8, scale=True),\n",
    "    v2.Resize((160, 160)),  # Измените размер под вашу модель\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "def compare_embeddings(embedding, threshold=0.7):\n",
    "    # 3. Поиск ближайшего embedding:\n",
    "    embedding_np = embedding.detach().numpy()\n",
    "    embedding_np = embedding_np.reshape(1, -1).astype('float32')  # Убедитесь, что форма и тип данных верны\n",
    "\n",
    "    distances, indices = index.search(embedding_np, k=1)\n",
    "    if distances > threshold:\n",
    "        return None\n",
    "    \n",
    "    # 4. Получение имени объекта по индексу:\n",
    "    closest_object_name = class_embedding_names[indices[0][0]]  # indices[0][0] - индекс ближайшего embedding\n",
    "\n",
    "    return closest_object_name\n",
    "    \n",
    "\n",
    "def detect_faces(image):\n",
    "    with torch.no_grad():  # Отключите вычисление градиентов во время инференса\n",
    "        result = detector_model([image], verbose=False)[0]\n",
    "    \n",
    "    return result.boxes.xyxy\n",
    "    \n",
    "        \n",
    "def recognize_faces(image, boxes):\n",
    "    cropped_faces = [\n",
    "        transform(image.crop(list(map(float,bbox))))\n",
    "        for bbox in boxes\n",
    "    ]\n",
    "    if cropped_faces:  # Убедитесь, что список не пустой\n",
    "        face_tensors = torch.stack(cropped_faces).to(device)\n",
    "        embeddings = classifier_model(face_tensors)\n",
    "    else:\n",
    "        embeddings = torch.empty(0).to(device)\n",
    "        \n",
    "    return embeddings\n",
    "\n",
    "def recognize_cropped_face(image):\n",
    "    tensor = transform(image).unsqueeze(0).to(device)\n",
    "    embedding = classifier_model(tensor)[0]\n",
    "    \n",
    "    return embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "277c31d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_classes.vgg_face2_face_recognition import VGGFace2FaceRecognitionDataset\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "def reverse_transform(image: torch.Tensor):\n",
    "    fn = v2.Compose([\n",
    "        v2.Normalize(mean=(-0.5 / 0.5, -0.5 / 0.5, -0.5 / 0.5), std=(1 / 0.5, 1 / 0.5, 1 / 0.5)),\n",
    "    ])\n",
    "    \n",
    "    return fn(image)\n",
    "\n",
    "dataset = VGGFace2FaceRecognitionDataset(\n",
    "    './recognition_dataset/images/test',\n",
    "    './recognition_dataset/labels/test',\n",
    ")\n",
    "known_face_map = dataset.known_face_map\n",
    "test_loader = DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2810e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name, path_list in known_face_map.items():\n",
    "    for file_path in path_list:\n",
    "        face_image = dataset._get_face_image(file_path)\n",
    "        embedding = recognize_cropped_face(face_image)\n",
    "        \n",
    "        known_embeddings.append(embedding)\n",
    "        class_embedding_names.append(class_name)\n",
    "        \n",
    "known_embeddings_detached = [tensor.detach().numpy() for tensor in known_embeddings] # отсоединяем каждый Tensor\n",
    "known_embeddings = np.array(known_embeddings_detached).astype('float32')\n",
    "index.add(known_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9676f3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 10000/167876 [23:29<6:10:45,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.635036496350365\n",
      "FAR:  0.025197480251974803\n",
      "FRR:  0.34206579342065796\n",
      "Precision:  0.9618355293048614\n",
      "Recall:  0.6499181334424887\n",
      "F1-Score:  0.7756946564885496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_loop = tqdm(dataset)\n",
    "total = 0\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for image, label in test_loop:\n",
    "    total += 1\n",
    "    boxes = detect_faces(image)\n",
    "    embeddings = recognize_faces(image, boxes)\n",
    "\n",
    "    for bbox, embedding in zip(boxes, embeddings):\n",
    "        x1 = int(bbox[0])\n",
    "        y1 = int(bbox[1])\n",
    "        x2 = int(bbox[2])\n",
    "        y2 = int(bbox[3])\n",
    "        \n",
    "        predicted_label = compare_embeddings(embedding)\n",
    "        if predicted_label == label:\n",
    "            tp += 1\n",
    "        elif predicted_label != label and predicted_label is not None:\n",
    "            fp += 1\n",
    "        elif predicted_label is None:\n",
    "            fn += 1\n",
    "    \n",
    "    if total > 10000:\n",
    "        break\n",
    "            \n",
    "recall = tp / (tp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "accuracy = (tp + tn) / total\n",
    "\n",
    "print('Accuracy: ', accuracy)\n",
    "print('FAR: ', fp/total)\n",
    "print('FRR: ', fn/total)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1-Score: ', f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
