{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f561e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    RUNNING_IN_COLAB = True\n",
    "    \n",
    "except ImportError:\n",
    "    drive = None\n",
    "    RUNNING_IN_COLAB = False\n",
    "\n",
    "if RUNNING_IN_COLAB:\n",
    "    os.system('git clone https://github.com/vekshinnikita/face_recognition.git /content/face_recognition')\n",
    "    os.chdir('/content/face_recognition') \n",
    "    \n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    zip_dataset_path = '/content/drive/MyDrive/recognition_dataset.zip'\n",
    "    destination_dataset_path = '/content/face_recognition/'\n",
    "    with zipfile.ZipFile(zip_dataset_path, 'r') as zip_ref:\n",
    "      zip_ref.extractall(destination_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d296507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Documents/university/Дипломы/Никита/project/venv_yolo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, using CPU instead\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from utils.system import get_available_device\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "\n",
    "device = get_available_device()\n",
    "\n",
    "KNOWN_FACES_PATH='./known_faces'\n",
    "\n",
    "# Model\n",
    "detector_model = YOLO(\"./best_models/yolo_decoder_best.pt\", verbose=False).to(device)\n",
    "classifier_model = InceptionResnetV1(pretrained='vggface2', num_classes=2, device=device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37abeffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_embeddings = list()\n",
    "class_embedding_names = list()\n",
    "\n",
    "index = faiss.IndexFlatL2(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb88a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.uint8, scale=True),\n",
    "    v2.Resize((160, 160)),  # Измените размер под вашу модель\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "def compare_embeddings(embedding, threshold=0.7):\n",
    "    # 3. Поиск ближайшего embedding:\n",
    "    embedding_np = embedding.detach().numpy()\n",
    "    embedding_np = embedding_np.reshape(1, -1).astype('float32')  # Убедитесь, что форма и тип данных верны\n",
    "\n",
    "    distances, indices = index.search(embedding_np, k=1)\n",
    "    if distances > threshold:\n",
    "        return None\n",
    "    \n",
    "    # 4. Получение имени объекта по индексу:\n",
    "    closest_object_name = class_embedding_names[indices[0][0]]  # indices[0][0] - индекс ближайшего embedding\n",
    "\n",
    "    return closest_object_name\n",
    "    \n",
    "\n",
    "def detect_faces(image):\n",
    "    with torch.no_grad():  # Отключите вычисление градиентов во время инференса\n",
    "        result = detector_model([image], verbose=False)[0]\n",
    "    \n",
    "    return result.boxes.xyxy\n",
    "    \n",
    "        \n",
    "def recognize_faces(image, boxes):\n",
    "    cropped_faces = [\n",
    "        transform(image.crop(list(map(float,bbox))))\n",
    "        for bbox in boxes\n",
    "    ]\n",
    "    if cropped_faces:  # Убедитесь, что список не пустой\n",
    "        face_tensors = torch.stack(cropped_faces).to(device)\n",
    "        embeddings = classifier_model(face_tensors)\n",
    "    else:\n",
    "        embeddings = torch.empty(0).to(device)\n",
    "        \n",
    "    return embeddings\n",
    "\n",
    "def recognize_cropped_face(image):\n",
    "    tensor = transform(image).unsqueeze(0).to(device)\n",
    "    embedding = classifier_model(tensor)[0]\n",
    "    \n",
    "    return embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "277c31d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_classes.vgg_face2_face_recognition import VGGFace2FaceRecognitionDataset\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "def reverse_transform(image: torch.Tensor):\n",
    "    fn = v2.Compose([\n",
    "        v2.Normalize(mean=(-0.5 / 0.5, -0.5 / 0.5, -0.5 / 0.5), std=(1 / 0.5, 1 / 0.5, 1 / 0.5)),\n",
    "    ])\n",
    "    \n",
    "    return fn(image)\n",
    "\n",
    "dataset = VGGFace2FaceRecognitionDataset(\n",
    "    './recognition_dataset/images/test',\n",
    "    './recognition_dataset/labels/test',\n",
    ")\n",
    "known_face_map = dataset.known_face_map\n",
    "test_loader = DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2810e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name, path_list in known_face_map.items():\n",
    "    for file_path in path_list:\n",
    "        face_image = dataset._get_face_image(file_path)\n",
    "        embedding = recognize_cropped_face(face_image)\n",
    "        \n",
    "        known_embeddings.append(embedding)\n",
    "        class_embedding_names.append(class_name)\n",
    "        \n",
    "known_embeddings_detached = [tensor.detach().numpy() for tensor in known_embeddings] # отсоединяем каждый Tensor\n",
    "known_embeddings = np.array(known_embeddings_detached).astype('float32')\n",
    "index.add(known_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9676f3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 421/167876 [00:53<5:52:31,  7.92it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loop\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university/Дипломы/Никита/project/venv_yolo/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university/Дипломы/Никита/project/dataset_classes/vgg_face2_face_recognition.py:120\u001b[39m, in \u001b[36mVGGFace2FaceRecognitionDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    117\u001b[39m image_path = \u001b[38;5;28mself\u001b[39m._get_image_path_by_index(index)\n\u001b[32m    118\u001b[39m label = \u001b[38;5;28mself\u001b[39m._get_class_name_by_index(index)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_face_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university/Дипломы/Никита/project/dataset_classes/vgg_face2_face_recognition.py:104\u001b[39m, in \u001b[36mVGGFace2FaceRecognitionDataset._get_face_image\u001b[39m\u001b[34m(self, image_path)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_face_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_path):\n\u001b[32m    103\u001b[39m   label_path = \u001b[38;5;28mself\u001b[39m._get_annotation_path(image_path)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m   image = Image.open(image_path)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_loop = tqdm(dataset)\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for image, label in test_loop:\n",
    "    total += 1\n",
    "    boxes = detect_faces(image)\n",
    "    embeddings = recognize_faces(image, boxes)\n",
    "\n",
    "    for bbox, embedding in zip(boxes, embeddings):\n",
    "        x1 = int(bbox[0])\n",
    "        y1 = int(bbox[1])\n",
    "        x2 = int(bbox[2])\n",
    "        y2 = int(bbox[3])\n",
    "        \n",
    "        predicted_label = compare_embeddings(embedding)\n",
    "        if predicted_label == label:\n",
    "            correct += 1\n",
    "            break\n",
    "            \n",
    "print('Accuracy: ', correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72c93c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6080760095011877\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', correct/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
